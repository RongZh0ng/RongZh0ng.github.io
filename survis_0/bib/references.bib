@misc{aich_leaf_2017,
  abstract = {In this paper, we investigate the problem of counting rosette leaves from an RGB image, an important task in plant phenotyping. We propose a data-driven approach for this task generalized over different plant species and imaging setups. To accomplish this task, we use state-of-the-art deep learning architectures: a deconvolutional network for initial segmentation and a convolutional network for leaf counting. Evaluation is performed on the leaf counting challenge dataset at CVPPP-2017. Despite the small number of training samples in this dataset, as compared to typical deep learning image sets, we obtain satisfactory performance on segmenting leaves from the background as a whole and counting the number of leaves using simple data augmentation strategies. Comparative analysis is provided against methods evaluated on the previous competition datasets. Our framework achieves mean and standard deviation of absolute count difference of 1.62 and 2.30 averaged over all five test datasets.},
  annote = {Comment: Workshop: ICCV 2017 Workshop on Computer Vision Problems in Plant Phenotyping (Code repository: https://github.com/p2irc/leaf\_count\_ICCVW-2017)},
  author = {Aich, Shubhra and Stavness, Ian},
  file = {arXiv Fulltext PDF:/Users/rong/Zotero/storage/GB76YSMM/Aich and Stavness - 2017 - Leaf Counting with Deep Convolutional and Deconvol.pdf:application/pdf;arXiv.org Snapshot:/Users/rong/Zotero/storage/2XIIDXSY/1708.html:text/html},
  keywords = {type:Calculating the number of leaves,counting rosette leaves,plant phenotyping,deconvolutional network,convolutional network},
  month = {aug,},
  doi = {https://doi.org/10.1109/ICCVW.2017.244},
  note = {arXiv:1708.07570 [cs] version: 2},
  publisher = {arXiv},
  title = {Leaf {Counting} with {Deep} {Convolutional} and {Deconvolutional} {Networks}},
  url = {http://arxiv.org/abs/1708.07570},
  urldate = {2023-04-16},
  year = {2017}
}

@article{chen_adversarial_2019,
  abstract = {Root imaging of a growing plant in a non-invasive, affordable, and effective way remains challenging. One approach is to image roots by growing them in a rhizobox, a soil-filled transparent container, imaging them with digital cameras, and segmenting root from soil background. However, due to soil occlusion and the fact that digital imaging is a 2D projection of a 3D object, gaps are present on the segmentation masks, which may hinder the extraction of finely grained root system architecture (RSA) traits. Herein, we develop an image inpainting technique to recover gaps from disconnected root segments. We train a patch-based deep fully convolutional network using a supervised loss but also use adversarial mechanisms at patch and whole root level. We use Policy Gradient method, to endow the model with large-scale whole root view during training. We train our model using synthetic root data. In our experiments, we show that using adversarial mechanisms at local and whole-root level we obtain a 72\% improvement in performance on recovering gaps of real chickpea data when using only patch-level supervision.},
  annote = {[TLDR] This work develops an image inpainting technique to recover gaps from disconnected root segments and shows that using adversarial mechanisms at local and whole-root level the authors obtain a 72\% improvement on recovering gaps of real chickpea data when using only patch-level supervision.},
  author = {Chen, Hao and Giuffrida, Mario Valerio and Doerner, Peter and Tsaftaris, Sotirios A.},
  doi = {10.1109/CVPRW.2019.00318},
  file = {Accepted Version:/Users/rong/Zotero/storage/RCSFIVLY/Chen et al. - 2019 - Adversarial Large-Scale Root Gap Inpainting.pdf:application/pdf},
  journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  month = {jun,},
  note = {Conference Name: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) ISBN: 9781728125060 Place: Long Beach, CA, USA Publisher: IEEE},
  pages = {2619--2628},
  keywords={type:Divided Leaf with Background,non-invasive,affordable,segmenting,image inpainting technique},
  title = {Adversarial {Large}-{Scale} {Root} {Gap} {Inpainting}},
  url = {https://ieeexplore.ieee.org/document/9025452/},
  urldate = {2023-04-16},
  year = {2019}
}

@misc{dobrescu_leveraging_2017,
  abstract = {The number of leaves a plant has is one of the key traits (phenotypes) describing its development and growth. Here, we propose an automated, deep learning based approach for counting leaves in model rosette plants. While state-of-the-art results on leaf counting with deep learning methods have recently been reported, they obtain the count as a result of leaf segmentation and thus require per-leaf (instance) segmentation to train the models (a rather strong annotation). Instead, our method treats leaf counting as a direct regression problem and thus only requires as annotation the total leaf count per plant. We argue that combining different datasets when training a deep neural network is beneficial and improves the results of the proposed approach. We evaluate our method on the CVPPP 2017 Leaf Counting Challenge dataset, which contains images of Arabidopsis and tobacco plants. Experimental results show that the proposed method significantly outperforms the winner of the previous CVPPP challenge, improving the results by a minimum of {\textasciitilde}50\% on each of the test datasets, and can achieve this performance without knowing the experimental origin of the data (i.e. in the wild setting of the challenge). We also compare the counting accuracy of our model with that of per leaf segmentation algorithms, achieving a 20\% decrease in mean absolute difference in count ({\textbar}DiC{\textbar}).},
  annote = {Comment: 8 pages, 3 figures, 3 tables},
  author = {Dobrescu, Andrei and Giuffrida, Mario Valerio and Tsaftaris, Sotirios A.},
  file = {arXiv Fulltext PDF:/Users/rong/Zotero/storage/ES47F59Q/Dobrescu et al. - 2017 - Leveraging multiple datasets for deep leaf countin.pdf:application/pdf;arXiv.org Snapshot:/Users/rong/Zotero/storage/STEIM4PP/1709.html:text/html},
  keywords = {type:Dividing plant roots and soil,phenotypes,deep learning,leaf segmentation,leaf count},
  month = {sep,},
  doi = {https://doi.org/10.1109/ICCVW.2017.243},
  note = {arXiv:1709.01472 [cs]},
  publisher = {arXiv},
  title = {Leveraging multiple datasets for deep leaf counting},
  url = {http://arxiv.org/abs/1709.01472},
  urldate = {2023-04-16},
  year = {2017}
}

@article{farjon_leaf_2021,
  abstract = {Leaf counting in potted plants is an important building block for estimating their health status and growth rate and has obtained increasing attention from the visual phenotyping community in recent years. Two novel deep learning approaches for visual leaf counting tasks are proposed, evaluated, and compared in this study. The first method performs counting via direct regression but using multiple image representation resolutions to attend leaves of multiple scales. The leaf count from multiple resolutions is fused using a novel technique to get the final count. The second method is detection with a regression model that counts the leaves after locating leaf center points and aggregating them. The algorithms are evaluated on the Leaf Counting Challenge (LCC) dataset of the Computer Vision Problems in Plant Phenotyping (CVPPP) conference 2017, and a new larger dataset of banana leaves. Experimental results show that both methods outperform previous CVPPP LCC challenge winners, based on the challenge evaluation metrics, and place this study as the state of the art in leaf counting. The detection with regression method is found to be preferable for larger datasets when the center-dot annotation is available, and it also enables leaf center localization with a 0.94 average precision. When such annotations are not available, the multiple scale regression model is a good option.},
  author = {Farjon, Guy and Itzhaky, Yotam and Khoroshevsky, Faina and Bar-Hillel, Aharon},
  doi = {10.3389/fpls.2021.575751},
  file = {Full Text:/Users/rong/Zotero/storage/CAU53WIP/Farjon et al. - 2021 - Leaf Counting Fusing Network Components for Impro.pdf:application/pdf},
  issn = {1664-462X},
  journal = {Frontiers in Plant Science},
  month = {jun,},
  keywords={type:Calculating the number of leaves,Leaf counting,visual phenotyping,deep learning,multiple scales},
  pages = {575751},
  shorttitle = {Leaf {Counting}},
  title = {Leaf {Counting}: {Fusing} {Network} {Components} for {Improved} {Accuracy}},
  url = {https://www.frontiersin.org/articles/10.3389/fpls.2021.575751/full},
  urldate = {2023-04-16},
  volume = {12},
  year = {2021}
}

@inproceedings{giuffrida_learning_2015,
  abstract = {Counting the number of leaves in plants is important for plant phenotyping, since it can be used to assess plant growth stages. We propose a learning-based approach for counting leaves in rosette (model) plants. We relate image-based descriptors learned in an unsupervised fashion to leaf counts using a supervised regression model. To take advantage of the circular and coplanar arrangement of leaves and also to introduce scale and rotation invariance, we learn features in a log-polar representation. Image patches extracted in this log-polar domain are provided to K-means, which builds a codebook in a unsupervised manner. Feature codes are obtained by projecting patches on the codebook using the triangle encoding, introducing both sparsity and speciﬁcally designed representation. A global, per-plant image descriptor is obtained by pooling local features in speciﬁc regions of the image. Finally, we provide the global descriptors to a support vector regression framework to estimate the number of leaves in a plant. We evaluate our method on datasets of the Leaf Counting Challenge (LCC), containing images of Arabidopsis and tobacco plants. Experimental results show that on average we reduce absolute counting error by 40\% w.r.t. the winner of the 2014 edition of the challenge –a counting via segmentation method. When compared to state-of-the-art density-based approaches to counting, on Arabidopsis image data ∼75\% less counting errors are observed. Our ﬁndings suggest that it is possible to treat leaf counting as a regression problem, requiring as input only the total leaf count per training image.},
  address = {Swansea},
  author = {Giuffrida, Mario Valerio and Minervini, Massimo and Tsaftaris, Sotirios},
  booktitle = {Procedings of the {Proceedings} of the {Computer} {Vision} {Problems} in {Plant} {Phenotyping} {Workshop} 2015},
  doi = {10.5244/C.29.CVPPP.1},
  file = {Giuffrida et al. - 2015 - Learning to Count Leaves in Rosette Plants.pdf:/Users/rong/Zotero/storage/2XTYHSYX/Giuffrida et al. - 2015 - Learning to Count Leaves in Rosette Plants.pdf:application/pdf},
  isbn = {978-1-901725-55-1},
  language = {en},
  pages = {1.1--1.13},
  publisher = {British Machine Vision Association},
  title = {Learning to {Count} {Leaves} in {Rosette} {Plants}},
  url = {http://www.bmva.org/bmvc/2015/cvppp/papers/paper001/index.html},
  keywords={type:Calculating the number of leaves,triangle encoding,learning-based approach,support vector regression},
  urldate = {2023-04-16},
  year = {2015}
}

@article{gong_pixel_2021,
  abstract = {The root architecture parameters are important to the study of plant growth state and the segmentation of plant roots is the key to the measurement of these parameters. Most existing methods use the threshold calculated by different algorithms to segment the roots in a grayscale image, which requires a low noise background. We designed a set of automatic equipment to record the roots images of rice seedlings planted in transparent bags. Those root images contain strong noise and it makes existing methods invalid in our circumstances. To solve the segmentation problem of rice roots under strong noise, we proposed a convolutional neural network based on U-Net and SE-ResNet. The root images were preprocessed and cropped into small patches to fit CNN input requirements. Experiments have shown that our method performs effectively in pixel-level segmentation of rice seedling roots that contain tiny lateral roots. Our method achieves an intersection over union (IoU) of 87.4\%. This method provides a new approach to automatic and fast pixel-level root segmentation, which is of great importance for the analysis of root morphology.},
  author = {Gong, Liang and Du, Xiaofeng and Zhu, Kai and Lin, Chenghui and Lin, Ke and Wang, Tao and Lou, Qiaojun and Yuan, Zheng and Huang, Guoqiang and Liu, Chengliang},
  doi = {10.1016/j.compag.2021.106197},
  file = {ScienceDirect Full Text PDF:/Users/rong/Zotero/storage/PDV2MHHS/Gong et al. - 2021 - Pixel level segmentation of early-stage in-bag ric.pdf:application/pdf;ScienceDirect Snapshot:/Users/rong/Zotero/storage/H62MBQ8X/S0168169921002143.html:text/html},
  issn = {0168-1699},
  journal = {Computers and Electronics in Agriculture},
  keywords = {type:Dividing plant roots and soil,Image processing, Image segmentation, Convolutional neural network, Deep-learning, Plant root},
  language = {en},
  month = {jul,},
  pages = {106197},
  title = {Pixel level segmentation of early-stage in-bag rice root for its architecture analysis},
  url = {https://www.sciencedirect.com/science/article/pii/S0168169921002143},
  urldate = {2023-04-17},
  volume = {186},
  year = {2021}
}

@article{kang_semantic_2021,
  abstract = {The growth and distribution of root system in the soil has an important influence on the growth of plants and is one of the important factors affecting crop production. However, the root system of plants is located in the dark and closed soil. Even if we can obtain high-definition root image from the complex soils, the interference of the soil particles on root system and the small difference of color between them will pose challenges for further root segmentation. In this experiment, the cotton mature root system is used as the research object. Based on the introduction of sub-pixel convolution DeepLabv3+ semantic segmentation model, we further added the attention mechanism to the model, assigning more weight to the pixel points of fine roots and their root hairs, and designed a semantic segmentation model of cotton roots in-situ image based on the attention mechanism. The experimental results show that the model has higher segmentation accuracy and operational efficiency than only introduces sub-pixel convolution DeepLabv3+ model, U-Net model and SegNet model. The precision value, recall value and F1-score are 0.9971, 0.9984 and 0.9937 respectively, and the IoU value of 161 untrained root image segmentation tasks was 0.9875. At the same time, we also performed segmentation experiments on the early cotton root images. The results show that the DeepLabv3+ model which only introduces sub-pixel convolution, U-Net model and SegNet model have poor segmentation effects. The semantic segmentation model based on attention mechanism proposed in this paper can be segmented accurately. The above results show that the proposed model can distinguish the cotton root system from the complex soil background accurately and has good segmentation effect. It can realize the accurate segmentation of root image in early and mature period in the process of cotton root growth, and provide important theoretical value and practical application reference for deep learning in plant root segmentation.},
  author = {Kang, Jia and Liu, Liantao and Zhang, Fucheng and Shen, Chen and Wang, Nan and Shao, Limin},
  doi = {10.1016/j.compag.2021.106370},
  file = {ScienceDirect Full Text PDF:/Users/rong/Zotero/storage/EED7Y9JU/Kang et al. - 2021 - Semantic segmentation model of cotton roots in-sit.pdf:application/pdf;ScienceDirect Snapshot:/Users/rong/Zotero/storage/2NRJ2JFW/S0168169921003872.html:text/html},
  issn = {0168-1699},
  journal = {Computers and Electronics in Agriculture},
  keywords = {type:Dividing plant roots and soil,Attention mechanism, Cotton root system, DeepLabv3+, Semantic segmentation},
  language = {en},
  month = {oct,},
  pages = {106370},
  title = {Semantic segmentation model of cotton roots in-situ image based on attention mechanism},
  url = {https://www.sciencedirect.com/science/article/pii/S0168169921003872},
  urldate = {2023-04-17},
  volume = {189},
  year = {2021}
}

@article{minervini_image-based_2014,
  abstract = {Plant phenotyping investigates how a plant's genome, interacting with the environment, affects the observable traits of a plant (phenome). It is becoming increasingly important in our quest towards efficient and sustainable agriculture. While sequencing the genome is becoming increasingly efficient, acquiring phenotype information has remained largely of low throughput. Current solutions for automated image-based plant phenotyping, rely either on semi-automated or manual analysis of the imaging data, or on expensive and proprietary software which accompanies costly hardware infrastructure. While some attempts have been made to create software applications that enable the analysis of such images in an automated fashion, most solutions are tailored to particular acquisition scenarios and restrictions on experimental design. In this paper we propose and test, a method for the segmentation and the automated analysis of time-lapse plant images from phenotyping experiments in a general laboratory setting, that can adapt to scene variability. The method involves minimal user interaction, necessary to establish the statistical experiments that may follow. At every time instance (i.e., a digital photograph), it segments the plants in images that contain many specimens of the same species. For accurate plant segmentation we propose a vector valued level set formulation that incorporates features of color intensity, local texture, and prior knowledge. Prior knowledge is incorporated using a plant appearance model implemented with Gaussian mixture models, which utilizes incrementally information from previously segmented instances. The proposed approach is tested on Arabidopsis plant images acquired with a static camera capturing many subjects at the same time. Our validation with ground truth segmentations and comparisons with state-of-the-art methods in the literature shows that the proposed method is able to handle images with complicated and changing background in an automated fashion. An accuracy of 96.7\% (dice similarity coefficient) was observed, which was higher than other methods used for comparison. While here it was tested on a single plant species, the fact that we do not employ shape driven models and we do not rely on fully supervised classification (trained on a large dataset) increases the ease of deployment of the proposed solution for the study of different plant species in a variety of laboratory settings. Our solution will be accompanied by an easy to use graphical user interface and, to facilitate adoption, we will make the software available to the scientific community.},
  author = {Minervini, Massimo and Abdelsamea, Mohammed M. and Tsaftaris, Sotirios A.},
  doi = {10.1016/j.ecoinf.2013.07.004},
  file = {ScienceDirect Full Text PDF:/Users/rong/Zotero/storage/YZGNSUPS/Minervini et al. - 2014 - Image-based plant phenotyping with incremental lea.pdf:application/pdf;ScienceDirect Snapshot:/Users/rong/Zotero/storage/ZRBSAG7D/S1574954113000691.html:text/html},
  issn = {1574-9541},
  journal = {Ecological Informatics},
  keywords = {type:Divided Leaf with Background,Machine learning, Active contour model, Agriculture, Gaussian mixture model, Phenotyping, Plant segmentation},
  language = {en},
  month = {sep,},
  pages = {35--48},
  series = {Special {Issue} on {Multimedia} in {Ecology} and {Environment}},
  title = {Image-based plant phenotyping with incremental learning and active contours},
  url = {https://www.sciencedirect.com/science/article/pii/S1574954113000691},
  urldate = {2023-04-16},
  volume = {23},
  year = {2014}
}

@article{noauthor_computer_2020,
  abstract = {Background Root system architecture (RSA) traits are of interest for breeding selection; however, measurement of these traits is difficult, resource intensive, and results in large variability. The advent of computer vision and machine learning (ML) enabled trait extraction and measurement has renewed interest in utilizing RSA traits for genetic enhancement to develop more robust and resilient crop cultivars. We developed a mobile, low-cost, and high-resolution root phenotyping system composed of an imaging platform with computer vision and ML based segmentation approach to establish a seamless end-to-end pipeline - from obtaining large quantities of root samples through image based trait processing and analysis. Results This high throughput phenotyping system, which has the capacity to handle hundreds to thousands of plants, integrates time series image capture coupled with automated image processing that uses optical character recognition (OCR) to identify seedlings via barcode, followed by robust segmentation integrating convolutional auto-encoder (CAE) method prior to feature extraction. The pipeline includes an updated and customized version of the Automatic Root Imaging Analysis (ARIA) root phenotyping software. Using this system, we studied diverse soybean accessions from a wide geographical distribution and report genetic variability for RSA traits, including root shape, length, number, mass, and angle. Conclusions This system provides a high-throughput, cost effective, non-destructive methodology that delivers biologically relevant time-series data on root growth and development for phenomics, genomics, and plant breeding applications. This phenotyping platform is designed to quantify root traits and rank genotypes in a common environment thereby serving as a selection tool for use in plant breeding. Root phenotyping platforms and image based phenotyping are essential to mirror the current focus on shoot phenotyping in breeding efforts.},
  copyright = {© 2020. This work is licensed under http://creativecommons.org/licenses/by/4.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
  doi = {10.1186/s13007-019-0550-5},
  file = {Full Text PDF:/Users/rong/Zotero/storage/GBJQBBS7/2020 - Computer vision and machine learning enabled soybe.pdf:application/pdf},
  language = {English},
  note = {Publisher: BioMed Central Section: Methodology},
  title = {Computer vision and machine learning enabled soybean root phenotyping pipeline},
  url = {https://www.proquest.com/docview/2357873409/abstract/F956421A0AA44DFAPQ/1},
  urldate = {2023-04-17},
  year = {2020}
  author = {Kevin G. Falk, Talukder Z. Jubery, Seyed V. Mirnezami, Kyle A. Parmley, Soumik Sarkar, Arti Singh, Baskar Ganapathysubramanian, Asheesh K. Singh},
  keywords={type:Dividing plant roots in specific situations,machine learning,root phenotyping system,computer vision},
}

@misc{ward_deep_2019,
  abstract = {Automated segmentation of individual leaves of a plant in an image is a prerequisite to measure more complex phenotypic traits in high-throughput phenotyping. Applying state-of-the-art machine learning approaches to tackle leaf instance segmentation requires a large amount of manually annotated training data. Currently, the benchmark datasets for leaf segmentation contain only a few hundred labeled training images. In this paper, we propose a framework for leaf instance segmentation by augmenting real plant datasets with generated synthetic images of plants inspired by domain randomisation. We train a state-of-the-art deep learning segmentation architecture (Mask-RCNN) with a combination of real and synthetic images of Arabidopsis plants. Our proposed approach achieves 90\% leaf segmentation score on the A1 test set outperforming the-state-of-the-art approaches for the CVPPP Leaf Segmentation Challenge (LSC). Our approach also achieves 81\% mean performance over all five test datasets.},
  annote = {Comment: British Machine Vision Conference (BMVC) 2018 Proceedings. CVPPP Workshop at BMVC 2018. Dataset available for download at: https://research.csiro.au/robotics/databases/synthetic-arabidopsis-dataset/},
  author = {Ward, Daniel and Moghadam, Peyman and Hudson, Nicolas},
  file = {arXiv Fulltext PDF:/Users/rong/Zotero/storage/SQVTNTTX/Ward et al. - 2019 - Deep Leaf Segmentation Using Synthetic Data.pdf:application/pdf;arXiv.org Snapshot:/Users/rong/Zotero/storage/ECDNBQYS/1807.html:text/html},
  keywords = {type:Calculating the number of leaves,Automated segmentation,domain randomisation,deep learning},
  month = {mar,},
  note = {arXiv:1807.10931 [cs]},
  publisher = {arXiv},
  title = {Deep {Leaf} {Segmentation} {Using} {Synthetic} {Data}},
  url = {http://arxiv.org/abs/1807.10931},
  urldate = {2023-04-17},
  doi = {https://doi.org/10.48550/ARXIV.1807.10931},
  year = {2019}
}

